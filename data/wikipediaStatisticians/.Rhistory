system("defaults write org.R-project.R force.LANG en_US.UTF-8")
sdfjkl
source("00-course-setup.r")
wd <- getwd()
source("00-course-setup.r")
wd <- getwd()
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
content <- read_html(url)
checkForServer()
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
checkForServer()
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$navigate(url)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
# set time frame
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- 'div.form-container:nth-child(6) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
css <- '#main > div:nth-child(1) > form:nth-child(4) > button:nth-child(14)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
devtools::install_github("ropensci/RSelenium")
library("RSelenium")
checkForServer()
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$navigate(url)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
# set time frame
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- 'div.form-container:nth-child(6) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
css <- '#main > div:nth-child(1) > form:nth-child(4) > button:nth-child(14)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
remDr$closeServer()
source("00-course-setup.r")
wd <- getwd()
checkForServer()
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
checkForServer()
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
checkForServer()
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$closeServer()
checkForServer()
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$navigate(url)
checkForServer()
# on a Mac: launch downloaded Selenium .jar file externally via Jar Launcher
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$navigate(url)
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
remDr$navigate(url)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
# set time frame
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- 'div.form-container:nth-child(6) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
css <- '#main > div:nth-child(1) > form:nth-child(4) > button:nth-child(14)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
output <- remDr$getPageSource(header = TRUE)
remDr$closeServer()
content <- read_html("iea-renewables.html", encoding = "utf8")
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
# open connection; Firefox window should pop up
remDr$open()
remDr$navigate(url)
# open web inspector tools (on a Mac with my setup, the following code did not work without it)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
# set time frame
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- 'div.form-container:nth-child(6) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
css <- '#main > div:nth-child(1) > form:nth-child(4) > button:nth-child(14)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
# close connection
remDr$closeServer()
# parse index table
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- head(tabs[[1]])
# add names
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
browseURL("http://www.starbucks.com/store-locator/search/location/chicago")
url <- "http://www.starbucks.com/store-locator/search/location/chicago"
checkForServer()
startServer()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr$navigate(url)
css <- 'span.icon:nth-child(2)'
click_elem <- remDr$findElement(using = 'css', value = css)
open_elem <- click_elem$clickElement() # click on button
css <- '#find_wf'
click_elem <- remDr$findElement(using = 'css', value = css)
open_elem <- click_elem$clickElement() # click on button
# download page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "starbucks-chicago.html")
# close connection
remDr$closeServer()
# import data
content <- read_html("starbucks-chicago.html", encoding = "utf8")
store_names <- html_nodes(content, ".store-name") %>% html_text()
store_addresses <- html_nodes(content, ".address li:nth-child(1)") %>% html_text()
# geocode and map stores
locations <- paste0(store_addresses, ", Chicago, IL")
pos <- geocode(locations, source = "google")
head(pos)
starbucks_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom=13, maptype="hybrid")
p <- ggmap(starbucks_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
browseURL("http://arxiv.org/help/api/index")
browseURL("http://arxiv.org/help/api/user-manual")
browseURL("http://export.arxiv.org/api/query?search_query=all:forecast")
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
source("00-course-setup.r")
wd <- getwd()
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
library(aRxiv)
ls("package:aRxiv")
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 1000, output_format = "data.frame")
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 1000)
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 500)
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20160720")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "20160720")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
lines(ymd_h(clinton_views$timestamp), clinton_views$views, col = "blue")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20160729")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "20160729")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
lines(ymd_h(clinton_views$timestamp), clinton_views$views, col = "blue")
library(httr)
x <- GET("https://google.com")
x
browseURL("http://www.omdbapi.com/")
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
# alternative search
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
browseURL("https://github.com/hrbrmstr/omdbapi")
browseURL("http://openweathermap.org/current")
browseURL("http://openweathermap.org/api")
apikey <- "&appid=42c7829f663f87eb05d2f12ab11f2b5d"
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Chicago,IL"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
setup_twitter_oauth(api_key,api_secret)
tweets <- searchTwitter(searchString = "Trump", n=25)
tweets_df <- twListToDF(tweets)
head(tweets_df)
names(tweets_df)
# get information about users
user <- getUser("RDataCollection")
load("twitter_auth.RData")
load("../rscraping-intro-duke-2/twitter_auth.RData")
filterStream("tweets_stream.json", track = c("Trump"), timeout = 10, oauth = twitCred)
tweets <- parseTweets("tweets_stream.json", simplify = TRUE)
names(tweets)
cat(tweets$text[1])
### -----------------------------
### simon munzert
### intro to web scraping with R
### -----------------------------
## preparations -----------------------
source("00-course-setup.r")
wd <- getwd()
url <- "https://www.google.de/?#q=list+breweries+chicago"
browseURL(url)
url <- "https://www.google.de/?#q=list+breweries+chicago"
browseURL(url)
url <- "http://thehopreview.com/blog/chicago-brewery-list"
content <- read_html(url, encoding = "utf8")
anchors <- html_nodes(content, css = "#block-yui_3_17_2_8_1438187725105_11398 p")
breweries <- html_text(anchors)
length(breweries)
head(breweries)
?get_map
lapply(p_needed, require, character.only = TRUE)
# install packages from CRAN
p_needed <- c("plyr", "dplyr", "stringr", "lubridate", "jsonlite", "httr", "xml2", "rvest", "devtools", "ggmap",  "networkD3", "RSelenium", "pageviews", "aRxiv", "twitteR", "streamR")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
install.packages(p_to_install)
}
lapply(p_needed, require, character.only = TRUE)
tempwd <- ("data/breweriesChicago")
dir.create(tempwd)
setwd(tempwd)
url <- "https://www.google.de/?#q=list+breweries+chicago"
url <- "https://www.google.de/?#q=list+breweries+chicago"
browseURL(url)
url <- "http://thehopreview.com/blog/chicago-brewery-list"
content <- read_html(url, encoding = "utf8")
content
anchors <- html_nodes(content, css = "#block-yui_3_17_2_8_1438187725105_11398 p")
anchors
breweries <- html_text(anchors)
breweries
length(breweries)
head(breweries)
breweries <- breweries[-1]
locations <- str_extract(breweries, "[[:digit:]].+?–")
locations
locations <- str_replace(locations, "–", ", Chicago, IL")
locations
locations <- locations[!is.na(locations)]
geocodeQueryCheck()
if (!file.exists("breweries_geo.RData")){
pos <- geocode(locations, source = "google")
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
pos
class(pos)
head(pos)
locations
head(breweries)
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom="auto", maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
guess_encoding(breweries)
repair_encoding(breweries)
# set temporary working directory
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
links
length(links)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
seq_along(links)
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
names
links
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
setwd(wd)
wd <- getwd()
setwd(wd)
setwd("../../")
# set temporary working directory
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
class(HTML)
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
## step 6: visualize connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
